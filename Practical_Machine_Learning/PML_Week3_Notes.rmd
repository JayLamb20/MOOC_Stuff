<center> <h2>Predictive Machine Learning - Week 3</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Video 3.1 Predicting with trees*

**A. Key ideas**

- iteratively split variables into groups
- evaluate homogeneity within each group
- split again if necessary
- pros:
    - easy to interpret
    - split again if necessary
- cons:
    - without pruning/cross-validation can lead to overfitting
    - harder to estimate uncertainty
    - results may be variable
    
**B. Decision Trees: Basic Algorithm**

1. start with all variables in one group
2. find the variable/split that best separates the outcomes
3. divide the data into two groups ("leaves") on that split ("node")
4. Within each split, find the best variable/split that separates the outcomes
5. Continue until the groups are too small or sufficiently "pure"

**C. Measures of impurity**

- misclassification error: 1 - probability that you're in the most common class
    - 0 = perfect purity
    - 0.5 = no purity (perfectly balanced between outcomes)
- GINI index (different from gini coefficient in econ)
    - 0 = perfect purity
    - 0.5 = no purity

**D. Measures of Impurity**

- deviance/information gain
    - comes from information theory
    - 0 = perfect purity
    - 1 = no purity

**E. An Example**

```{r sec1trees, eval=FALSE}
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

## exploratory plot
qplot(Petal.Width, Sepal.Width, color = Species, data=training)

##let's fit the tree model
library(caret)
modFit <- train(Species ~., method = "rpart", data=training) ## rpart is the r thing for using trees
print(modFit$finalModel)

## plot the tree
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n=TRUE, all = TRUE, cex=0.8)

## Prettier Tree Plot with rattle package
library(rattle)
fancyRpartPlot(modFit$finalModel)

## predictions
predict(modFit, newdata=testing) ## this will write out the class predicted
```

**F. Notes and Further Resources**

- classification trees are non-linear models
    - they use interactiosn between variables
    - data transformations may be less improtant (monotone transformations)
    - trees can also be used for regression problems (continuous outcome)
        - use RMSE as a measure of impurity
- note that there are multiple tree building options in R both in the carety package
    - party
    - rpart
    - tree
- see "Classification and regression trees" book linked to

####*Video 3.2 Bagging*

**A. What is Bagging?**

- bagging = "bootstrap aggregating"
- this is a model averaging approach

**B. Basic Idea**

1. resample cases and recalculate predictions
2. Average or majority vote
-Notes:
    - similar bias
    - reduced variance (relative to single model)
    - more useful for non-linear functions)
    
**C. Example with Ozone data**

```{r sec2ozone, eval=FALSE}
library(ElemStatLearn); data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone,]

## bagged loess
ll <- matrix(NA, nrow=10, ncol=155) ## create a matrix with 10 rows, 155 cols

## loop through, sample with replacement, pick 10 datasets
## reorder the dataset every time by ozone
## fit a loess curve each time (a smoother, similar to a spline modelfit)
## span is a measure of smoothness
## predict for each loess curve, outcome for new dataset
## predict from loess0 curve from the i-th resample
## fill in the i-th row of ll with predictions for i-th run
for(i in 1:10){
    ss <- sample(1:dim(ozone)[1], replace=T)
    ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone,]
    loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
    ll[i,] <- predict(loess, newdata=data.frame(ozone=1:155))
    }

## plotting the averages

plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10){lines(1:155, ll[i,], col="grey", lwd=2)} ## plot the individual forecasts
lines(1:155, apply(ll,2,mean),col="red", lwd=2) ## take column means, plot
```

**C. Some Notes on Bagging**

- the bagged estimate will have lower variability
- some stuff in train:
    - method = bagEarth
    - method = treebag
    - method = bagFDA

**D. More bagging in caret**

- this is a very advanced use

```{r sec3bags, eval=FALSE}
## put predictors in their own data frame
predictors = data.frame(ozone=ozone$ozone)

## same with variable of interest
temperature = ozone$temperature

## set up the bagging
treebag <- bag(predictors, temperature, B = 10,
               bagControl = bagControl(fit = ctreeBag$fit,
                                    predict = ctreeBag$pred,
                                    aggregate = ctreeBag$aggregate))


```


**E. parts of Bagging**

- ```ctreebag$fit```
    - takes the dataframe(x) and outcome(y)
    - uses the ctree() function to run a regression tree on the data
- ```ctreeBag$pred```
    - takes an object from the fit, and a new dataset x
    - calculates the tree response from the object and new data
    - calculates the probability matrix
    - either returns observed level or predicted response
- ```ctreeBag$aggregate```
    - gets all the predictions from the model fits
    - binds them into one data matrix (each row is one model prediction)
    - takes median prediction from all model fits across bootstrapped samples
    
**F. Notes and Further Resources**

- Notes:
    - bagging is most useful for nonlinear models
    - often used with trees - an extension is random forests
    - several models use bagging in caret's train function
- Further resources:
    - bagging (link)
    - bagging and boosting (link)
    - Elements of Statistical Learning (link)
####*Video 3.3 Random Forests*

**A. **

####*Video 3.4 Boosting*

**A. **

####*Video 3.5 Model Based Prediction*

**A. **