<center> <h2>Data Science Capstone - Forum Notes</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

###*The Idea*

There are a lot of great discussions going on on those forums. Seems worthwhile (as a research exercise and thought experiment) to record the good ones here. This will be an invaluable resource if I end up having to retake the capstone in June.


####*General Discussion*

A. [Deterministic Lookup Nature of Next Word Prediction](https://class.coursera.org/dsscapstone-003/forum/thread?thread_id=99)

- NWP = "Next Word Prediction"
    - More than just a VLOOKUP
    - Context matters
    - think about filling in "I am going to ___"
    - Is that going to be "a", "the"? or "Boston"?
- You aren't picking <b>THE</b> answer
    - suggest the top few (maybe 3-5)
- basic probabilistic prediction is probably a pretty good start
- You need to work out what to do with phrases the corpus hasn't seen
    - At some point, you move from exact matches to "close matches"
    - That's when you go from making it do "something" to making it succeed

B. [Best way to adjust back-off or interpolation to generalize better](https://class.coursera.org/dsscapstone-003/forum/thread?thread_id=111)

- commenters note that simple n-gram predicts well for n-grams that have already been seen, but is awful for ones that haven't been seen
- one approach would be the [back-off model](http://en.wikipedia.org/wiki/Katz%27s_back-off_model)
- [these lecture notes from Maryland](https://class.coursera.org/dsscapstone-003/forum/thread?thread_id=111) look promising too
- "Have you considered that having impossible answers in the corpus may be polluting your results? This is basically the idea that rather than having one corpus for each and every occasion, you migght get better results with an environment-specific corpus?"
- there is potential for "drawing inspiration from the related idea of sequential patterns-itemsets where the order does matter"
    - "...turning our idea of the data from ngrams = prediction to ngram3out + ngram2out + ngram1out = prediction should give scope for flexible analysis"
    - effectively a 4 column tabular set of data rather than a 2 column
    - "If there is a general principle underlying this of extracting as much informaiton from the raw data as possible, and the place of individuals items is a possible lever to do something clever with"
- GO THROUGH THE STANFORD NLP LECTURES

C. [Grammar Recognition](https://class.coursera.org/dsscapstone-003/forum/thread?thread_id=25)

- "You can build a really good application almost completely ignoring most of the aspects of the natural language"
- Jeff Leek --> "I agree with the idea of starting simple and seeing how well you can do. If your algorithm knew a lot about grammar you may be able to improve the predictions, but I think building simple rules using things like n-grams and then adding on from there is a good way to go"

D. [starting point: understand NL first](http://www.myreaders.info/10_Natural_Language_Processing.pdf)

- check out [this pdf](http://www.myreaders.info/10_Natural_Language_Processing.pdf)

E. [CTA Advice, Starting Out](https://class.coursera.org/dsscapstone-003/forum/thread?thread_id=4)

<b>Make a Plan</b>

- break what you need to do into stages, break those down again. Get bite-sized pieces to do.
- Break everything down into smaller pieces, ask yourself how pieces are contributing to your goals.
- Use the discussion boards heavily

<b> Draw Comfort From What You Know</b>

- there are obvious parallels between this project and Peer Assignment 2 in Reproducible Research. Don't get scared!
- there's no specific thing in caret we can just dump the data into, but the mechanism for predicting will be be the better for keeping machine learning in mind
- the entire data set is massive, but remember inferential statistics....you might learn a lot just from samples
- Michael A. Wise --> "Without this capstone, I would have been still mired in decision forests and regressions."
    - "NLP is an incredible important branch of ML and many people taking the capstone just don't get that"
- Be sure to remember what the actual goal is....don't get sucked into the wormhole of NLP

- "It doesn't really matter how clever and accurate your algorithm is....just deliver a working application"

- *Key Point* --> "My second advice would be to ignore NLP for a while. Different NLP techniques can be useful but, espeically when you've never worked with any of these, can become your worst enemy."
- Be familiar with R garbage collector and other internals of the language. 

- "Related Advice: start early on the tasks and get a simple application working (even if it doesn't produce great predictions at first). Then, when the pressure is off, focus on adding improvements. It really is a great feeling when you "awaken the computer and it starts producing good predictions!"

- Start with a small dataset
- <b>"The rubric for the data product is marking accuracy against Twitter. So a third option is to optimize on Twitter (and given the size of the data sets I am sure there will be some discussion of optimization in other threads)</b>
- *Another Key Point* --> "This specific app is about meeting the project rubric. You could do many other projects with the ideas involved in this app. When in doubt check agianst the rubric in the navigation area, keeping in mind there are also quizzes."

F. [Free private git repositories](https://class.coursera.org/dsscapstone-003/forum/thread?thread_id=28)

- The professors will ask you to take all your stuff off of GitHub
- Maybe set this up on BitBucket?

####*Task 0 - Understanding the problem*

A. [Profanity Filtering](https://class.coursera.org/dsscapstone-003/forum/thread?thread_id=12)

- A good approach would be to keep the profane words in your model until the final second. At the very end, just substitute in something like "$%^&*"
- "Having a dynamic predictor is cool though, although not necessary"
- *Great Advice* --> "...interpret the project brief honestly but narrowly, to end up with something that you know can be delivered on schedule. However, leave enough flexibility in the code structure that you can try other things if the main objective goes faster than expected."
- [profane words in a bunch of languages](https://github.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/)

B. [I am lost](https://class.coursera.org/dsscapstone-003/forum/thread?thread_id=91) 

- [a paper on low-level ngrams](http://www.uni-koblenz-landau.de/en/campus-koblenz/fb4/west_old/theses/modified-kneser-ney-smoothing-on-top-of-generalized-language-models-for-next-word-prediction)
- another key paper, called [text mining infrastructure in R](http://www.jstatsoft.org/v25/i05/paper)
- *Key thought* "With hindisght, the ```tm()``` package has been on of my greatest sources of wasting my time until now. Use it to create your corpus, to remove profanity, numbers, and whitespace etc., but stop there. Use the tokenizing functions Maciej has graciously made public. Use grep to find frequency distributions. Follow the relevant parts of the Stanford NLP course. All this should help to get started."
- consider some work in the [stylo package](http://cran.r-project.org/web/packages/stylo/stylo.pdf)?

- Benjamin Uminsky's notes on tokenizing:
    - Start off by cerating three different ngram frequency tables, one for unigrams, one for bigrams, etc. (maybe up to 4 or 5)
    - **TM is a horrific memory hog**
    - <FONT COLOR="RED">"My preference is to use the ```stylo``` package. The syntax for creating your specified gram/token is very easy and direct. You can then run a table() on your ngrams to get your frequency table. From there you can just arrange your frequency table by the word or phrase counts and begin exploring the most common to least common words or phrases"</FONT>
    - It's fun to figure out how many words you need for % of total words/phrases
    - A bunch of words with count=1 will be in you corpus. These are not that useful
- some useful functions:
    - ```tolower()```, ```str_replace()```, ```gsub()```, ```removeWords()```
