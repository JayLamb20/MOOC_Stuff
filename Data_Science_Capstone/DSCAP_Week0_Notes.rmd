<center> <h2>Data Science Capstone - Week 0</h2> </center>
<center> <h3>---------------------------------------</h3> </center>

####*Resource 1: NLP Concepts and Definitions

- [Automatic summarization](https://en.wikipedia.org/wiki/Automatic_summarization) = providing a readable summary of a chunk of text.
    - often used to provide summaries of text of a known type, such as articles in the financial section of a newspaper.
- [cache language models](http://en.wikipedia.org/wiki/Cache_language_model) = A class of statistical language models. These occur in the NLP subfield of computer science and assign probabilities to given sequences of words by means of a probability distribution. Unlike non-cache alternatives, these models update as new data are observed. In other words, they rely on the ever-growing corpus of text for training data, rather than predetermined "global" probabilities of word occurence in the native language.
- [Coreference resolution](https://en.wikipedia.org/wiki/Coreference) = Identify and understand referential relationships between words. e.g. link pronouns to the nouns they reference. 
- [corpora](http://en.wikipedia.org/wiki/Text_corpus) = a corpus or text corpus (plural *corpora*) is a large and structured set of texts. They are used to do statistical analysis and hypothesis testing.
    - from Wikipedia: "the analysis and processing of various types of corpora are also the subject of much work in computational linguistics, speech recognition and machine translation, where they are often used to create hidden Markov models for part of speech tagging and other purposes"
- [Discourse analysis](https://en.wikipedia.org/wiki/Discourse_analysis) = Essentially, this is the task of understanding the discourse structure of connected text. So, for example, you might be interested in classifying sentences as elaboration, explanation, or contrast. Discourse analysis might also include recognition of speech acts like yes-no questions, content questions, statements, and assertions
- [frequency list](http://en.wikipedia.org/wiki/Word_lists_by_frequency) = a list of a language's words grouped by frequency of occurrence within some given text corpus, either by levels or as a ranked list
- [Hidden Markov Models](http://en.wikipedia.org/wiki/Hidden_Markov_model) = Statistical markov models in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. Ann HMM can be presented as the simplest dynamic Bayesian network.In a hidden Markov model, the state is not directly visible, but output, dependent on the state, is visible. Each state has a probability distribution over the possible output tokens, so the sequence of tokens generated by an HMM gives some information about the sequence of states.
- [Machine translation](https://en.wikipedia.org/wiki/Machine_translation) = The task of automatically translating text from one language into another. This problem is "AI-complete", in the sense that it requires all of the the different types of human knowledge to solve correctly. 
- [maxent classifiers](http://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/) = A probabilistic classifier which belongs to the class of exponential models. Unlike naive bayes, the maximum entorpy does not assume that the features are conditionaly independent of each other. Models with the largest entropy are chosen. Maximum Entropy is used when we can't assume the conditional independence of the features. See the link for more info.
- [Morpheme](https://en.wikipedia.org/wiki/Morpheme) = The smallest grammatical unit in a language. These may be free (i.e. stand alone as words, e.g. "town", "dog") or bound (i.e. appear only as word parts, e.g. "un-", "-able")
- [Morphological segmentation](https://en.wikipedia.org/wiki/Morphology_(linguistics)) = The task of breaking words down into their component morphemes and classifying those morphemes. English morphology is so simple that this is often unnecessary for English corpora.
- [naive bayes classifiers](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) = a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. It is useful in text categorization, the problem of judging documents as belonging to one category or the other. All naive Bayes classifiers assume that the value of a particular feature is independent of teh value of any other feature, given the class variable.
- [Named entity recognition (NER)](https://en.wikipedia.org/wiki/Named_entity_recognition) = This is an exercise in classification. Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization).
- [n-gram modeling](http://en.wikipedia.org/wiki/N-gram#n-gram_models) = An n-gram is a contiguous sequence of *n* items from a given sequence of text or speech (unigram, bigram, trigram). An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence. An n-gram model scales with a well-understood space-time tradeoff.
- [Natural language generation](https://en.wikipedia.org/wiki/Natural_language_generation) = Conversion of digital information into readable human language.
- [Natural language search](https://en.wikipedia.org/wiki/Natural_language_user_interface) = Allowing digital search via natural language queries.
- [Natural language understanding](https://en.wikipedia.org/wiki/Natural_language_understanding) = Extraction of semantics (in a computer-friendly form such as first-order logics) from natural language text. Many different natural language combinations can be used to express the same semantics.
- [Optical character recognition](https://en.wikipedia.org/wiki/Optical_character_recognition) = Determining the corresponding text given an image representing printed text (think gestalt).
- [Part-of-speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging) = Given a sentence, determine the part of speech for each word. This can be very difficult. Languages with little inflectional morphology, such as English are particularly prone to ambiguity in part of speech.
- [Parsing](https://en.wikipedia.org/wiki/Parsing) = Determine the grammatical structure of a given sentence. This is surprisingly much more difficult for a computer than for a human
- [probabilistic dependency](http://dmt.r-forge.r-project.org/) = Modeling which exploits statistical dependencies between co-occurring observations. Couldn't find a good definition of this...see the link for more documentation.
- [Query expansion](https://en.wikipedia.org/wiki/Query_expansion) = the process of reformulating a seed query to improve retrieval performance in information retrieval operations. 
- [Question Answering](https://en.wikipedia.org/wiki/Question_answering) = Determine the answer to a human-language question. This is especially difficult with open-ended questions.
- [Relationship Extraction](https://en.wikipedia.org/wiki/Relationship_extraction) = Identify the relationships among named entities (e.g. who is the wife of whom). 
- [Sentence breaking](https://en.wikipedia.org/wiki/Sentence_breaking) = For a chunk of text, find where the sentences break. There are obvious breaks with punctuation marks, but these marks can serve other purposes (like in abbreviations)
- [sentence tokenization](http://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)) = The process of breaking a stream of text up into words, phrases, or other meaningful elements called tokens. The list of tokens becomes input for further processing such as parsing or text mining.Tokenization can be thought of as a form of text segmentation.
- [sentiment analysis](http://en.wikipedia.org/wiki/Sentiment_analysis) = Extracting subjective information from text. This is typically accomplished by polar grouping, but placing sentiment on a continuous spectrum is possible.
- [Speech recognition](https://en.wikipedia.org/wiki/Speech_recognition) = Determine textual representation of audio speech.
- [Speech segmentation](https://en.wikipedia.org/wiki/Speech_segmentation) = A subtask within speech recognition, this involves separating a sound clip of a person speaking into words. Separating the words is necessary prior to interpretation of them.
- [Stemming](https://en.wikipedia.org/wiki/Stemming) = In linguistic morphology, this refers to the process of reducing inflected (or derived) words to their word stem, base, or root form. A stemmer for English, as an example, should identify the string "cats" (and "catlike", "catty", etc.) as based on the root "cat". The stem does not have to be a valid root. A stemming algorithm might correctly reduce "argue", "argued", "argues", and "arguing" to the stem "argu".
- [Text Simplification](https://en.wikipedia.org/wiki/Text_simplification) = an operation used in NLP to modify, enhance, classify or otherwise process an existing corpus of human-readable text in such a way that the grammar and structure of the prose is greatly simplified, while the underlying meaning and information remains the same. 
- [Topic segmentation](https://en.wikipedia.org/wiki/Topic_segmentation) = Separate a chunk of text into smaller segments, grouped by topic.
- [Truecasing](https://en.wikipedia.org/wiki/Truecasing) = The problem, in NLP, of determining the proper capitalization of words where such information is unavailable. This aids in other tasks like named entity recognition, machine translation, and Automatic Content Extraction.
- [vector-space models of meaning](http://en.wikipedia.org/wiki/Vector_space_model) = A vector space model is an algebraic model for representing text documents as vectors of identifiers, such as, for example, index terms. This modeling is used in information filtering, information retrieval, indexing and relevancy rankings. 
- [Word segementation](https://en.wikipedia.org/wiki/Word_segmentation) = Separate a chunk of continuous text into separate words. This is trivially easy in English, but in other languages like Chinese and Japanese where there are no clear word boundaries, this is much more difficult.
- [Word sense disambiguation](https://en.wikipedia.org/wiki/Word_sense_disambiguation) = A given word's meaning changes, and these changes are dependent on context. Word sense disambiguation involves identifying the context and selecting which meaning makes them most sense. Tagging words with emotional metadata might aid in this task. 
- [Zipf's Law](http://en.wikipedia.org/wiki/Zipf%27s_law) = an empirical law stating that many types of data studied in the physical and social sciences can be approximated by a Zipfian distribution, one of a family of related discrete power law probability distributions
    - It states that "given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the freuqncy table. Thus, the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc."
    - to observe Zipf's law, plot log(rank order) vs. log(frequency)
    - In human languages, word frequencies have a very heavy-tailed distribution, and can therefore be modeled reasonably well by a Zipf distribution with an *s* close to 1:
<center><img src="http://upload.wikimedia.org/math/f/7/e/f7ea651d890ccb3249ecd1c8eded8869.png"></center>


####*Resource 2: NLP Wikipedia Page*

[Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing)

**A. History**

- Early NLP efforts:
    - [Turing Test](https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence)
    - [Georgetown experiment](https://en.wikipedia.org/wiki/Georgetown-IBM_experiment)
    - [ALPAC Report](https://en.wikipedia.org/wiki/ALPAC)
    - [SHRDLU](https://en.wikipedia.org/wiki/SHRDLU)
    - [ELIZA](https://en.wikipedia.org/wiki/ELIZA)
- "Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing."
    - due to Moore's Law-driven increases in computing power and "gradual lessening of the dominance of Chomskyan theories of linguistics"
- early work here: decision treees
- later on, part of speech tagging enabled the use of hidden Markov models in NLP
- "...most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems."

**B. NLP Using Machine Learning**

- the general idea: use machine learning algorithms (grounded in statistical inference) to automatically learn a language's "rules" through the analysis of large corpora of typical real-world examples
- "Increasingly, research [in this area] has focused on statistical models, which make soft, probabilistic decisions based on attached real-valued weights to each input feature"
- advantages of ML in this space:
    - the algorithms will naturally be attracted to the most important features, something that is far from guaranteed in hand-written rule formation
    - tools of statistical inference handle misspelled or unfamiliar inputs much more gracefully than systems of handwritten rules
    - key --> "systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules, which is a far more difficult task."
- learning in NLP is studied within the subfield of Natural Language Learning (NLL)

**C. Major tasks in NLP**

- [Automatic summarization](https://en.wikipedia.org/wiki/Automatic_summarization), [Coreference resolution](https://en.wikipedia.org/wiki/Coreference), [Discourse analysis](https://en.wikipedia.org/wiki/Discourse_analysis), [Machine translation](https://en.wikipedia.org/wiki/Machine_translation), [Morphological segmentation](https://en.wikipedia.org/wiki/Morphology_(linguistics)), [Named entity recognition (NER)](https://en.wikipedia.org/wiki/Named_entity_recognition), [Natural language generation](https://en.wikipedia.org/wiki/Natural_language_generation), [Natural language search](https://en.wikipedia.org/wiki/Natural_language_user_interface), [Natural language understanding](https://en.wikipedia.org/wiki/Natural_language_understanding), [Optical character recognition](https://en.wikipedia.org/wiki/Optical_character_recognition), [Part-of-speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging), [Parsing](https://en.wikipedia.org/wiki/Parsing), [Query expansion](https://en.wikipedia.org/wiki/Query_expansion), [Question Answering](https://en.wikipedia.org/wiki/Question_answering), [Relationship Extraction](https://en.wikipedia.org/wiki/Relationship_extraction), [Sentence breaking](https://en.wikipedia.org/wiki/Sentence_breaking), [Speech recognition](https://en.wikipedia.org/wiki/Speech_recognition), [Speech segmentation](https://en.wikipedia.org/wiki/Speech_segmentation), [Stemming](https://en.wikipedia.org/wiki/Stemming), [Text-proofing](https://en.wikipedia.org/wiki/Text-proofing), [Text Simplification](https://en.wikipedia.org/wiki/Text_simplification), [Topic segmentation](https://en.wikipedia.org/wiki/Topic_segmentation), [Truecasing](https://en.wikipedia.org/wiki/Truecasing), [Word segementation](https://en.wikipedia.org/wiki/Word_segmentation), [Word sense disambiguation](https://en.wikipedia.org/wiki/Word_sense_disambiguation)

####*Resource 3: NLP CRAN Page*

- [NLP packages for R](http://cran.r-project.org/web/views/NaturalLanguageProcessing.html)

####*Resource 4: Text Mining Infrastructure in R*

Feinerer, Ingo, Kurt Hornik, and David Meyer. "Text Mining Infrastructure in R." *Journal of Statistical Software* 25, no. 5 (2008): 1-54. Retrieved from: http://www.jstatsoft.org/v25/i05/paper.

####*Video 0.1 - Your Are a Data Scientist Now*

**A. Notes**

- faculty will be with us all the way

####*Video 0.2 - Welcome to the Capstone!*

**A. Introduction**

- partnership with [SwiftKey](http://swiftkey.com/en/)


####*Video 0.3 - welcome from SwiftKey*

**A. Introduction**

- Steps:    
    - analysis
    - cleaning
    - tokenization
- "We want you to turn this corpus into a predictive system: How cool is that?"

####*Video 0.4 - Intro to Task 0: Understanding the Problem*

**A. Introduction**

- basic goal: build predictive model for natural-language English text


####*Video 0.5 - Introduction to Task 1: Getting and Cleaning the Data

- Main tasks:
    - tokenization
    - profanity filtering
- Think of a strategy for dealing with lots of stuff:
    - misspelling
    - punctuation

####*Video 0.6 - Regular Expression Part 1-2*

**A. Intro to Regex**

- a combination of literals and metacharacters
- Draw an analogy with natural language, think of literal text forming the words of this language, and the metacharacters

**B. Literals**

- Simplest pattern consists only of literals. Words that match exactly

**C. Simplicity**

- simplest pattern consists only of literals: a match occurs if the sequence occurs anywhere in the text

**D. Alternatives**

- we need a way to express:
    - whitespace word boundaries
    - sets of literals
    - the beginning and end of a line
    - alternatives ("war" or "peace") Metacharacters to the rescue!

**E. Metacharacters**

- "^" will match start of a line
- "$" represents the end of the line
- We can list a set of characters we will accept at a given point in the match
    - [Bb][Uu][Ss][Hh] will match all letters of "Bush", regardless of what's actually capitalized
- ranges of characters or letters:
    - [a-z] --> any characters a to z, lowercase
    - [a-zA-Z] --> all characters
    - [0-9] any numbers
- carat inside the class says to match anything NOT in the class
    - [^?.]$ will find all the lines that don't end with a "?" or "."
- "." is used to refer to any character
    - searching on "9.11" will give you any place with 9, any character, 11
- the "|" is a logical or
    - "flood|fire" will grab all lines with either flood or fire
    - you can include a bunch of alternatives
    - note that alternatives (stuff in the "or") can be regex too (not just literals)
- you could use parentheses to constrain alternatives
    - ^(Good|Bad] looks only for sentences *starting* with Good or Bad
- The question mark indicates that the indicated expression is optional
- to match a "." as a literal period, we need to escape it, as in "\."
- the * and + signs are metacharacters used to indicate repetition
    - * means "any number, including none, of the item"
    - + means "at least one of the item"
- { and } are referred to as interval quantifiers; they let us specify the minimum and maximum number of matches of an expression
    - {m,n} --> "at least m times but not more than n matches"
    - {m} --> "exactly m matches"
    - {m,} --> "at least m matches"
- In most implementations of regular expressions, the parentheses not only limit the scope of alternatives divided by a "|", but also can be used to "remember" text matched by the subexpression enclosed
    - +([a-zA-Z]+) +\1 +
    - the above translates to: "a space followed by at least one but maybe more characters followed by at least one space followed by the same match from inside the parentheses --> would pick up "night night", "blah blah"
- the * is "greedy", so it always matches the longest possible string that satisfies the regular expression

**F. Summary**

- Regex are used in many different languages: not unique to R
- Regular expressions are composed of literals and metacharacters that represent sets or classes of characters/words
- Text processing via regular expressions is a very powerful way to extract data from "unfriendly" sources (not all data comes as a CSV file)
- used with the functions ```grep```, ```grepl```, ```sub```, ```gsub``` and others 




    

