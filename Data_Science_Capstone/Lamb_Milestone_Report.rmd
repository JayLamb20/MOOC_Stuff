---
title: "Data Science Capstone - Milestone Report"
author: "James Lamb"
output: 
    html_document:
        theme: journal
---

##I. Project Description

This document contains a brief description of my initial efforts for the Capstone project in the Johns Hopkins [data science specialization](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage), a collection of MOOCs provided in partnership with Coursera.

##II. Data Summary

####*A. Source and Description*

The raw data (which can be downloaded [here]("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")), contain text corpora from three source types (blogs, news, Twitter) in four languages (English, Finnish, German, Russian). Only the English corpora are used for this particular project.

####*B. Exploratory Data Analysis*

I present some preliminary summary statistics below. Only limited data cleaning was done in preparing these summaries. More thorough processing (described in the next section) will be implemented prior to deploying the final app.

```{r eda, eval=TRUE, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}

    ## Package loading
    library(tm); library(pryr); library(knitr); library(gridExtra); library(stringr);

    ## read in the data (use UTF-8 encoding so it gets through all the weird characters) http://en.wikipedia.org/wiki/UTF-8
    types <- list("blogs", "news", "twitter")
    
    for (i in 1:length(types)){
        con <- file(paste0(".\\Coursera-SwiftKey\\final\\en_US\\en_US.",types[[i]],".txt"), open="rb")
        assign(paste0("data_",types[[i]]), readLines(con, encoding="UTF-8"))
        close(con); rm(con);
    }

    ## Convert to Corpus, fix issues with weird characters [http://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs]

    ## Some useful functions

        ## define a preprocessing function (http://stat.ethz.ch/R-manual/R-patched/library/base/html/regex.html)
        preproc <- function(x){
            x <- stringr::str_replace_all(x, "[^[:graph:]]", " ") ## remove everything except graphical characeters (mitigate errors)
            x <- tm::stripWhitespace(x) ## strip out whitespace
            x <- tolower(x) ## convert to lowercase
            x <- tm::removeNumbers(x) ## remove numbers
            x
        }

    ## Word count function (assumes character vector as input)
    word_count  <- function(x){

        ## function for counting
        line_count <- function(x){
            length(unlist(strsplit(x, " ")))
        }
        
        ## loop and count
        word_vec <- as.numeric(lapply(x,line_count))
        
        ## print this vector of counts out
        word_vec
    }

    ## Do some data processing/cleaning
    data_blogs <- preproc(data_blogs)
    data_news <- preproc(data_news)
    data_twitter <- preproc(data_twitter)

    ## Function to summarize a corpus that comes in as a text vector
    summarize_text <- function(corpora = list(), types=list()){
        
        ## initialize a list holding vectors of summary stats
        statlist <- list(NULL,NULL,NULL,NULL,NULL)
        
        ## initialize the dataframe to hold everything
        sumstats <- data.frame(rep(0,5))
        
        for (i in 1:length(types)){
            
            ## extract this vector from the list
            x <- corpora[[i]]
            
            ## initialize a vector to store the summary states
            temp <- rep(0,5)
            
            ## calculate object size
            temp[1] <- pryr::object_size(x)/1000
            
            ## line counts
            temp[2] <- length(x)
            
            
            ## summary stats
            wordcounts <- word_count(x)
            
            temp[3] <- sum(wordcounts)
            temp[4] <- mean(wordcounts)
            temp[5] <- max(wordcounts)
            
#             ## store in a vector
#             statlist[[i]] <- gsub(" ", "", format(round(temp,0), big.mark=",", scientific=F))
            
            ## append to the dataframe
            sumstats[,i] <- gsub(" ", "", format(round(temp,0), big.mark=",", scientific=F))
            }
        
        rownames(sumstats) <- c("Size in Memory (kB)", "Number of Lines", "Number of Total Words", "Avg. Words Per Line", "Longest Line (# words)")
        names(sumstats) <- unlist(types)
        
        ## print the table (save to global environment too)
        stats_table <<- kable(sumstats) #stats_table <- grid.table(sumstats)
        stats_table

    }

    ## create the summary table
    summarize_text(corpora=list(data_blogs,data_news,data_twitter), types = list("blogs", "news", "twitter"))

```

A few interesting observations from this preliminary summarizing effort:

1. The *Twitter* corpus contains many small, independent sentences (i.e. short lines).
2. The *blogs* corpus is most likely to have long lines of related prose (i.e. high words per line)
3. The *news* corpus seems, at first glance, more similar to the blogs than to Twitter

The plots below (based only on the first 50,000 lines of each corpus) provide some additional word-level insights:

```{r edaPlots, eval=TRUE, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE, results='asis'}

    ## load packages
    suppressPackageStartupMessages(library(googleVis)); library(tm);

    ## Blogs

        ## Subset Out a piece of the data for graphing
        smpl <- 50000L ##10000L
        g_blogs <- head(data_blogs,n=smpl)
        gt_blogs <- tolower(scan_tokenizer(g_blogs))
        freqtable_blogs <- sort(table(gt_blogs), decreasing=TRUE)
        
        ## get a list of the words
        top25_blogs <- head(names(freqtable_blogs),n=25)
    
        ## convert to data frame
        df_blogs <- data.frame(freqtable_blogs)
        df_blogs$word <- rownames(df_blogs)
        df_blogs <- df_blogs[,c(2,1)] ## reorder
        names(df_blogs) <- c("word", "freq")
    
        ## recode everything below the top 10 to "other"
        df_blogs[which((df_blogs$word %in% top25_blogs)==FALSE),]$word <- "other"
            df_blogs$freq <- as.numeric(df_blogs$freq)
            df_blogs$word <- as.factor(df_blogs$word)
    
        ## aggregate
        blogs_agg <- aggregate(freq ~ word, data=df_blogs, na.rm=TRUE, sum)
            blogs_agg <- blogs_agg[order(-blogs_agg$freq),]
    
        ## create the googleVis pie chart
        p_blogs <- gvisPieChart(blogs_agg,options = list(title = "Blog Corpus: Top 25 Words", width = 300, height = 400))
        #plot(p_blogs)
        
    ## News

        ## Subset Out a piece of the data for graphing
        g_news <- head(data_news,n=smpl)
        gt_news <- tolower(scan_tokenizer(g_news))
        freqtable_news <- sort(table(gt_news), decreasing=TRUE)
        
        ## get a list of the words
        top25_news <- head(names(freqtable_news),n=25)
    
        ## convert to data frame
        df_news <- data.frame(freqtable_news)
        df_news$word <- rownames(df_news)
        df_news <- df_news[,c(2,1)] ## reorder
        names(df_news) <- c("word", "freq")
    
        ## recode everything below the top 10 to "other"
        df_news[which((df_news$word %in% top25_news)==FALSE),]$word <- "other"
            df_news$freq <- as.numeric(df_news$freq)
            df_news$word <- as.factor(df_news$word)
    
        ## aggregate
        news_agg <- aggregate(freq ~ word, data=df_news, na.rm=TRUE, sum)
            news_agg <- news_agg[order(-news_agg$freq),]
    
        ## create the googleVis pie chart
        p_news <- gvisPieChart(news_agg,options = list(title = "News Corpus: Top 25 Words", width = 300, height = 400))

    ## Twitter

        ## Subset Out a piece of the data for graphing
        g_twitter <- head(data_twitter,n=smpl)
        gt_twitter <- tolower(scan_tokenizer(g_twitter))
#         gt_twitter <- tolower(MC_tokenizer(data_twitter)) ## gets past UTF problems for now. Solve later.
        freqtable_twitter <- sort(table(gt_twitter), decreasing=TRUE)
        
        ## get a list of the words
        top25_twitter <- head(names(freqtable_twitter),n=25)
    
        ## convert to data frame
        df_twitter <- data.frame(freqtable_twitter)
        df_twitter$word <- rownames(df_twitter)
        df_twitter <- df_twitter[,c(2,1)] ## reorder
        names(df_twitter) <- c("word", "freq")
    
        ## recode everything below the top 10 to "other"
        df_twitter[which((df_twitter$word %in% top25_twitter)==FALSE),]$word <- "other"
            df_twitter$freq <- as.numeric(df_twitter$freq)
            df_twitter$word <- as.factor(df_twitter$word)
    
        ## aggregate
        twitter_agg <- aggregate(freq ~ word, data=df_twitter, na.rm=TRUE, sum)
            twitter_agg <- twitter_agg[order(-twitter_agg$freq),]
    
        ## create the googleVis pie chart
        p_twitter <- gvisPieChart(twitter_agg,options = list(title = "Twitter Corpus: Top 25 Words", width = 300, height = 400))


    ## create the final plot
    p_temp <- gvisMerge(p_blogs,p_news, horizontal=TRUE)
    p_final <- gvisMerge(p_temp, p_twitter, horizontal=TRUE)
    #plot(p_final)

    ## print the html: slidify
    print(p_final, "chart")

## NEED SOME CODE WITH PLOTS OF THE DATA
# http://voyant-tools.org/ ??
# https://blogs.princeton.edu/etc/2012/08/16/see-text-in-whole-new-waytext-visualization-tools/
```

The plots provide a few additional insights at the unigram level:

1. Possessive language (e.g. "I", "you", "my") is more prevalent in the Twitter and Blogs corpora.
2. On a related note, News tends to involve more impersonal language (e.g. "he", "his")
3. The blogs corpus shows the *word* "s" at the 18th most popular, suggesting that the data have some unforeseen structure that my preprocessing and first-pass tokenization is failing to pick up

##III. The Prediction Algorithm

####*A. Pre-Processing*

Prior to performing any prediction, the corpus will need to be processed into a usable format. First, I will use some of the utilities in the ```tm``` package to strip out unnecessary white space and remove special characters. Next, I'll use my own function to break the lines into sentences and then to break sentences into n-grams.

For a general-use, all-audiences application like the one proposed for this project, it's advisable to filter profane words. Rather than omitting profane words from the corpus, which would hurt predictive accuracy, I've decided to leave them in but replace them with something like "%%%%" in the final output. I've identified several lists of profane English words online. The definition of what is and is not profanity is highly personal, so in an attempt to bring some objectivity to the problem, I will create my own profanity list which includes all words appearing on at least half of the candidate lists.

Because the model will need to provide rapid, reactive predictions in a web browser, it needs to be lightweight. For this reason, I will conduct more detailed exploratory analysis to ensure that only reasonably common phrases/words are stored in the final model as candidates for prediction.


####*B. Baseline Model*

The baseline prediction algorithm will be a simple one-word-ahead [n-gram model](http://en.wikipedia.org/wiki/N-gram) model. This model will take as its inputs a string of words from the user and will predict the next word in the sequence.

For unobserved input combinations, I will rely on a simple [back-off model](http://en.wikipedia.org/wiki/Katz%27s_back-off_model). Essentially, this involves systematically reducing *n* until a reliable prediction of the next word can be made.

For example: "The building looks" --> "building looks" --> "looks".

The model will be trained on a training set (85% of total corpus) and its predictive accuracy will be tested on a holdout sample (15%).

####*C. Potential Enhancements*

The goal for the baseline model is just to predict better than a random draw from the corpus might. To improve performance, I will explore the following enhancements:

- Tagging of [stopwords](http://en.wikipedia.org/wiki/Stop_words) (such as *the*, *it*, and *which*) to improve the back-off model
- Use of more advanced data storage options like [hash tables](http://cran.r-project.org/web/packages/hash/hash.pdf) to improve the speed of the app
- Some manual fixes of [common misspellings](http://www.oxforddictionaries.com/words/common-misspellings)
- Examination of strategies for choosing between a model based on Twitter data and one based on news and blogs (on the basis of user input)
- Replacement of contractions with their consituent bigrams (e.g. "don't" --> "do not")

A few Twitter-specific enhancements:

- Thoughtful and appropriate handling of [emojis](http://en.wikipedia.org/wiki/Emoji) (most likely dropping them)
- Appropriate treatment of common acronyms like *RT*, *btw*, etc.

##IV. Shiny App Design

####*A. Project Scope*

I do not have ambitious plans for the user interface on the [shiny app](http://shiny.rstudio.com/). Following the advice of some of the Community TAs, my focus will be primarily on fulfilling the requirements in the project rubric.  I plan to create a simple shiny app which gives users an entry text box, a submit button, and some predictions printed to the page.

##V. Final Thoughts

This is, admittedly, a very basic, unambitious project description. As a result of several time pressures, I've fallen a bit behind on Tasks 1-4 in the project. I hope to go well beyond the level of sophistication described in this report, but my plan at the moment is to focus on the basic construction and make enhancements as time permits.

Thank you for reading my proposal. I look forward to hearing your thoughts.